<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Introduction on Machine Learning Notes</title><link>https://poryoung.github.io/machine-learning-notes/</link><description>Recent content in Introduction on Machine Learning Notes</description><generator>Hugo -- gohugo.io</generator><atom:link href="https://poryoung.github.io/machine-learning-notes/index.xml" rel="self" type="application/rss+xml"/><item><title/><link>https://poryoung.github.io/machine-learning-notes/docs/notes/BackPropagation/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://poryoung.github.io/machine-learning-notes/docs/notes/BackPropagation/</guid><description>BackPropagation # \(\) Single-hidden Layer Feedforward Neural Network # input output output layer $\beta_j=\sum_{h=1}^{q} \omega_{h j} b_{h}$ $\hat{y}_{j}^{k}=f\left(\beta_j-\theta_j\right)$ hidden layer $\alpha_h=\sum_{i=1}^{d} v_{i h} x_{i}$ $b_{n}=f\left(\alpha_{i h}-\gamma_{h}\right)$ input layer $x_i$ . LossFunction # 累计误差$E_k$，目标$min E_k$
$$ E_{k}=\frac{1}{2} \sum_{j=1}^{l}\left(\hat{y}_{j}^{k}-y_{j}\right)^{2} $$
Iterative Equations1 # 隐层到输出层连接边权值变化
$$ \Delta \omega_{h}=-\eta \frac{\partial E_{k}}{a \omega_{k j}} $$</description></item><item><title/><link>https://poryoung.github.io/machine-learning-notes/docs/notes/Perceptron/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://poryoung.github.io/machine-learning-notes/docs/notes/Perceptron/</guid><description>Perceptron # \(\) Perceptron is a kind of binary classification model.
Loss Function # $$ L ( \omega, b ) =\sum_{i \in M}-y_{i} \frac{\left|\omega x_{i}+b\right|}{|\omega|} $$
Target # $$ \min L ( \omega, b ) $$
Method # 随机梯度下降法 # 目标梯度
$$ \nabla_{\omega} L=-\sum_{x_{i} \in M} y_{i} x_{i} $$
$$ \nabla_{b} L=-\sum_{x_{i} \in M} y_{i} $$
选取初值$\omega_0$, $b_0$
迭代式
$$ \omega \leftarrow \omega+\eta \cdot y_{i} x_{i} $$</description></item></channel></rss>