'use strict';(function(){const b={cache:!0};b.doc={id:'id',field:['title','content'],store:['title','href','section']};const a=FlexSearch.create('balance',b);window.bookSearchIndex=a,a.add({id:0,href:'/machine-learning-notes/docs/notes/',title:"Notes",section:"Docs",content:"   \\(\\)  Notes #    Back Propagation  BackPropagation # \\(\\) Single-hidden Layer Feedforward Neural Network # input output output layer $\\beta_j=\\sum_{h=1}^{q} \\omega_{h j} b_{h}$ $\\hat{y}_{j}^{k}=f\\left(\\beta_j-\\theta_j\\right)$ hidden layer $\\alpha_h=\\sum_{i=1}^{d} v_{i h} x_{i}$ $b_{n}=f\\left(\\alpha_{i h}-\\gamma_{h}\\right)$ input layer $x_i$ . LossFunction # 累计误差$E_k$，目标$min E_k$ $$ E_{k}=\\frac{1}{2} \\sum_{j=1}^{l}\\left(\\hat{y}_{j}^{k}-y_{j}\\right)^{2} $$ Iterative Equations1 # 隐层到输出层连接边权值变化 $$ \\Delta \\omega_{h}=-\\eta \\frac{\\partial E_{k}}{a \\omega_{k j}} $$   Perceptron  Perceptron # \\(\\) Perceptron is a kind of binary classification model. Loss Function # $$ L ( \\omega, b ) =\\sum_{i \\in M}-y_{i} \\frac{\\left|\\omega x_{i}+b\\right|}{|\\omega|} $$ Target # $$ \\min L ( \\omega, b ) $$ Method # 随机梯度下降法 # 目标梯度 $$ \\nabla_{\\omega} L=-\\sum_{x_{i} \\in M} y_{i} x_{i} $$ $$ \\nabla_{b} L=-\\sum_{x_{i} \\in M} y_{i} $$ 选取初值$\\omega_0$, $b_0$ 迭代式 $$ \\omega \\leftarrow \\omega+\\eta \\cdot y_{i} x_{i} $$   "}),a.add({id:1,href:'/machine-learning-notes/docs/notes/BackPropagation/',title:"Back Propagation",section:"Notes",content:"BackPropagation #    \\(\\)   Single-hidden Layer Feedforward Neural Network #       input output     output layer $\\beta_j=\\sum_{h=1}^{q} \\omega_{h j} b_{h}$ $\\hat{y}_{j}^{k}=f\\left(\\beta_j-\\theta_j\\right)$   hidden layer $\\alpha_h=\\sum_{i=1}^{d} v_{i h} x_{i}$ $b_{n}=f\\left(\\alpha_{i h}-\\gamma_{h}\\right)$   input layer $x_i$ .      LossFunction #  累计误差$E_k$，目标$min E_k$\n$$ E_{k}=\\frac{1}{2} \\sum_{j=1}^{l}\\left(\\hat{y}_{j}^{k}-y_{j}\\right)^{2} $$\nIterative Equations1 #  隐层到输出层连接边权值变化\n$$ \\Delta \\omega_{h}=-\\eta \\frac{\\partial E_{k}}{a \\omega_{k j}} $$\n$$ \\begin{aligned} \\frac{\\partial E_{k}}{\\partial \\omega_{h j}} \u0026amp;=\\frac{\\alpha E_{k}}{\\partial \\hat{y}_{j}^{k}} \\cdot \\frac{\\partial \\hat{y}_{j}^{k}}{\\partial \\beta_{j}} \\cdot \\frac{\\partial \\beta_{j}}{\\partial \\omega_{h j}} \\newline \u0026amp;=\\left(\\hat{y}_{j}^{k}-y_{j}^{k}\\right) f^{\\prime}\\left(\\beta_{j}-\\theta_{j}\\right) \\cdot b_{h} \\end{aligned} $$\n输出层阈值变化\n$$ \\Delta \\theta_{j}=-\\eta \\frac{\\partial E_k}{\\partial \\theta_{j}} $$\n$$ \\begin{aligned} \\frac{\\partial E_k}{\\partial \\theta_{j}} \u0026amp;=\\frac{\\partial E_{k}}{\\partial \\hat{y}_{j}^{k}} \\cdot \\frac{\\partial \\hat{y}_{j}^{k}}{\\partial \\theta_j} \\newline \u0026amp;=\\left(\\hat{y}_{j}^{k}-y_{j}^{k}\\right) \\cdot f^{\\prime}\\left(\\beta_{i}-\\theta_{j}\\right) \\cdot(-1)=g_{j} \\end{aligned} $$\n输入层到隐层连接边权值变化\n$$ \\Delta v_{i h}=-\\eta \\frac{\\partial E k}{\\partial V_{i h}} $$\n$$ \\begin{aligned} \\frac{\\partial E_k}{\\partial v_{i h}}\u0026amp;=\\sum_{j=1}^{l} \\frac{\\alpha E_{k}}{\\partial \\hat{y}_{j}^{k}} \\cdot \\frac{\\partial \\hat{y}_{j}^{k}}{\\partial \\beta_{j}} \\cdot \\frac{\\partial \\beta_{j}}{\\partial b_{h}} \\cdot \\frac{\\partial b_{h}}{\\partial \\alpha_{h}} \\cdot \\frac{\\partial \\alpha_{h}}{\\partial v_{i h}} \\newline \u0026amp;=\\sum_{j=1}^{k}\\left(\\hat{y}_{j}^{k}-y_{5}^{k}\\right) \\cdot f^{\\prime}\\left(\\beta_{j}-\\theta_{j}\\right) \\cdot \\frac{\\partial \\beta_{j}}{\\partial b_{j}} \\cdot \\frac{\\partial b_{h}}{\\partial \\alpha_{h}} \\cdot \\frac{\\partial \\alpha_{h}}{\\partial v_{i h}} \\newline \u0026amp;=\\left[\\sum_{j=1}^{l}\\left(\\hat{y}_{j}^{k}-y_{j}^{k}\\right) \\cdot f^{\\prime}\\left(\\beta_{j}-\\theta_{j}\\right) \\cdot \\omega_{h j}\\right] \\cdot \\frac{\\partial b_{h}}{\\partial \\alpha_{h}} \\cdot \\frac{\\partial \\alpha_{h}}{\\partial v_{i h}} \\end{aligned} $$\n通常可以根据经验公式 $m=log_2 ( n )$， ( $m$为隐层节点数，$n$为输入层节点数 ) 得到隐层应节点数。  Examples #  Show an Example: fit $0.5*(cos(x)\u0026#43;1)$  测试 max_iter=10000, error=0.0001, same_error_times=10 iterated 10000/10000 times, error 0.2785599852590231.    Show an Example: fit $0.5*cos(x_1)*sin(x_2)$  import numpy as np import pandas as pd import matplotlib.pyplot as plt # 二维训练集样本 x = np.array((np.linspace(-7, 7, 200), np.linspace(-7, 7, 200))).T y = np.expand_dims((np.cos(x[:, 0]) + np.sin(x[:, 1])) * 0.5, 1) bp = BackPropagation(q=3, lr_1=0.5, lr_2=0.6) bp.fit(x, y, max_iter=1000, error=0.0001, same_error_times=10) iterated 1000/1000 times, error is 19.441564975483463, covergent 1 times.  ax = plt.subplot(111, projection=\u0026#34;3d\u0026#34;) ax.plot3D(x[:, 0], x[:, 1], y[:, 0], c=\u0026#34;w\u0026#34;) ax.plot3D(x_test[:, 0], x_test[:, 1], Y_Y[:, 0], c=\u0026#34;b\u0026#34;) [\u0026lt;mpl_toolkits.mplot3d.art3d.Line3D at 0x7fb286c26340\u0026gt;]  # 二维训练集样本 x = np.array((np.linspace(-7, 7, 200), np.linspace(-7, 7, 200))).T y = np.expand_dims((np.cos(x[:, 0]) + np.sin(x[:, 1])) * 0.5, 1) bp = BackPropagation(q=5, lr_1=0.8, lr_2=0.6) bp.fit(x, y, max_iter=1000, error=0.0001, same_error_times=10) iterated 568/1000 times, error is 14.466125682677891, covergent 9 times.  # 二维测试集样本 x_test = np.array((np.linspace(-9, 9, 200), np.linspace(-9, 9, 200))).T Y_Y = bp.predict(x_test) ax = plt.subplot(111, projection=\u0026#34;3d\u0026#34;) ax.plot3D(x[:, 0], x[:, 1], y[:, 0], c=\u0026#34;w\u0026#34;) ax.plot3D(x_test[:, 0], x_test[:, 1], Y_Y[:, 0], c=\u0026#34;b\u0026#34;) [\u0026lt;mpl_toolkits.mplot3d.art3d.Line3D at 0x7fb286b2af70\u0026gt;]  start_time = time.time() bp.fit( x, y, max_iter=1000000, error=0.00001, same_error_times=200, Rh=bp.Rh, Thej=bp.Thej, Vih=bp.Vih, Whj=bp.Whj, ) end_time = time.time() print(\u0026#34;training costs {} s\u0026#34;.format(end_time - start_time)) iterated 21841/1000000 times, error is 13.866926271017798, covergent 199 times. training costs 573.6911239624023 s  # 二维测试集样本 x_test = np.array((np.linspace(-9, 9, 200), np.linspace(-9, 9, 200))).T Y_Y = bp.predict(x_test) ax = plt.subplot(111, projection=\u0026#34;3d\u0026#34;) ax.plot3D(x[:, 0], x[:, 1], y[:, 0], c=\u0026#34;w\u0026#34;) ax.plot3D(x_test[:, 0], x_test[:, 1], Y_Y[:, 0], c=\u0026#34;b\u0026#34;) [\u0026lt;mpl_toolkits.mplot3d.art3d.Line3D at 0x7fb28699fcd0\u0026gt;]    Code #  Show Code #%% import numpy as np import pandas as pd import matplotlib.pyplot as plt # %% path = \u0026#34;/home/ias/workdir/ml-primary/ml-notes/data/watermelon_data3.0.csv\u0026#34; data = pd.read_csv(path) data.head() # %% from sklearn import preprocessing enc = preprocessing.OneHotEncoder() a = np.array(enc.fit_transform(data.iloc[:, :7]).toarray()) b = np.array(data.iloc[:, 7:9]) X = np.c_[a, b] y = np.array(enc.fit_transform(data.iloc[:, 9:]).toarray()) #%% class BackPropagation: def __init__(self, q=1, lr_1=0.1, lr_2=0.1) -\u0026gt; None: self.q = q self.lr_1 = lr_1 self.lr_2 = lr_2 def sigmoid(self, v, the): return 1 / (1 + np.exp(-(v - the))) def fit( self, X, Y, max_iter=50, error=0.001, same_error_times=5, Rh=None, Thej=None, Vih=None, Whj=None, ): \u0026#34;\u0026#34;\u0026#34;fit AI is creating summary for fit Args: X ([type]): [N * d] Y ([type]): [N * m] max_iter (int, optional): [description]. Defaults to 50. \u0026#34;\u0026#34;\u0026#34; # init N, d = np.shape(X) m = np.shape(Y)[1] Rh = np.random.random(self.q) if Rh is None else Rh Thej = np.random.random(m) if Thej is None else Thej Vih = np.random.random((d, self.q)) if Vih is None else Vih Whj = np.random.random((self.q, m)) if Whj is None else Whj error_list = [] old_Ek = 0 cur = 0 sn = 0 while cur \u0026lt; max_iter: Ek = np.zeros(N) for k in range(N): # calculate Bh Ah = np.zeros(self.q) Bh = np.zeros(self.q) for h in range(self.q): Ah[h] = np.dot(X[k], Vih[:, h]) Bh[h] = self.sigmoid(Ah[h], Rh[h]) # calculate Yj Pj = np.zeros(m) Yj = np.zeros(m) Gj = np.zeros(m) for j in range(m): Pj[j] = np.dot(Bh, Whj[:, j]) Yj[j] = self.sigmoid(Pj[j], Thej[j]) # calculate Gj Gj[j] = Yj[j] * (1 - Yj[j]) * (Y[k][j] - Yj[j]) # calculate Eh Eh = np.zeros(self.q) for h in range(self.q): Eh[h] = Bh[h] * (1 - Bh[h]) * np.dot(Gj, Whj[h, :]) # update Whj += self.lr_1 * np.reshape(np.kron(Bh, Gj), (self.q, m)) Vih += self.lr_2 * np.reshape(np.kron(X[k], Eh), (d, self.q)) Thej += -self.lr_1 * Gj Rh += -self.lr_2 * Eh # calculate Ek Ek[k] = 0.5 * np.sum(np.power(Yj - Y[k], 2)) if abs(old_Ek - sum(Ek)) \u0026lt; error: sn += 1 if sn \u0026gt;= same_error_times: break else: old_Ek = sum(Ek) error_list.append(old_Ek) sn = 0 cur += 1 print( \u0026#34;\\riterated {}/{} times, error is {}, covergent {} times.\u0026#34;.format( cur, max_iter, old_Ek, sn ), end=\u0026#34;\u0026#34;, ) print(\u0026#34;\u0026#34;, end=\u0026#34;\\n\u0026#34;) print( \u0026#34;\\rFinished, iterated {}/{} times, error is {}, covergent {} times.\u0026#34;.format( cur, max_iter, old_Ek, sn ), end=\u0026#34;\u0026#34;, ) self.Rh = Rh self.Thej = Thej self.Vih = Vih self.Whj = Whj self.error_list = error_list def predict(self, x_test): Y_Y = np.zeros((np.shape(x_test)[0], np.shape(self.Whj)[1])) for i in range(len(x_test)): A_H = np.dot(x_test[i], self.Vih) B_V = np.array( [self.sigmoid(A_H[h], self.Rh[h]) for h in range(len(A_H))] ) P_J = np.dot(B_V, self.Whj) Y_O = np.array( [self.sigmoid(P_J[j], self.Thej[j]) for j in range(len(P_J))] ) Y_Y[i] = Y_O return Y_Y # %% # 训练集样本 x = np.array([np.linspace(-7, 7, 200)]).T y = (np.cos(x) + 1) / 2 bp = BackPropagation(q=3, lr_1=0.3) bp.fit(x, y, max_iter=1000, error=0.0001, same_error_times=10) # %% # 测试集样本 x_test = np.array([np.linspace(-9, 9, 120)]).T # 测试集结果 # y_predict = network.feedforward(x_test) Y_Y = np.zeros(len(x_test)) for i in range(len(x_test)): A_H = x_test[i] * bp.Vih B_V = [bp.sigmoid(v, bp.Rh) for v in A_H] Y_M = np.shape(bp.Whj)[1] Y_J = np.zeros(Y_M) Y_O = np.zeros(Y_M) for j in range(Y_M): Y_J[j] = np.dot(B_V, bp.Whj[:, j]) Y_O[j] = bp.sigmoid(Y_J[j], bp.Thej[j]) Y_Y[i] = Y_O # %% nbp = BackPropagation(q=3, lr_1=0.3) nbp.fit( x, y, max_iter=1000, error=0.0001, same_error_times=10, Rh=bp.Rh, Thej=bp.Thej, Vih=bp.Vih, Whj=bp.Whj, ) x_test = np.array([np.linspace(-9, 9, 120)]).T Y_Y = nbp.predict(x_test) plt.plot(x, y, \u0026#34;r\u0026#34;, x_test, Y_Y, \u0026#34;*\u0026#34;) # %% # 二维训练集样本 import time start_time = time.time() x = np.array((np.linspace(-7, 7, 200), np.linspace(-7, 7, 200))).T y = np.expand_dims((np.cos(x[:, 0]) + np.sin(x[:, 1])) * 0.5, 1) bp = BackPropagation(q=5, lr_1=0.8, lr_2=0.4) bp.fit(x, y, max_iter=1000000, error=0.0001, same_error_times=100) end_time = time.time() print(\u0026#34;training costs {} s\u0026#34;.format(end_time - start_time)) # %% # 二维测试集样本 x_test = np.array((np.linspace(-9, 9, 200), np.linspace(-9, 9, 200))).T Y_Y = bp.predict(x_test) # %% ax = plt.subplot(111, projection=\u0026#34;3d\u0026#34;) ax.plot3D(x[:, 0], x[:, 1], y[:, 0], c=\u0026#34;w\u0026#34;) ax.plot3D(x_test[:, 0], x_test[:, 1], Y_Y[:, 0], c=\u0026#34;b\u0026#34;) # %% start_time = time.time() nbp = BackPropagation(q=5, lr_1=0.3, lr_2=0.1) nbp.fit( x, y, max_iter=1000000, error=0.00001, same_error_times=200, Rh=bp.Rh, Thej=bp.Thej, Vih=bp.Vih, Whj=bp.Whj, ) end_time = time.time() print(\u0026#34;training costs {} s\u0026#34;.format(end_time - start_time)) # %%       南瓜书 PumpkinBook\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   "}),a.add({id:2,href:'/machine-learning-notes/docs/notes/Perceptron/',title:"Perceptron",section:"Notes",content:"Perceptron #     \\(\\)  Perceptron is a kind of binary classification model.\n Loss Function #  $$ L ( \\omega, b ) =\\sum_{i \\in M}-y_{i} \\frac{\\left|\\omega x_{i}+b\\right|}{|\\omega|} $$\nTarget #  $$ \\min L ( \\omega, b ) $$\nMethod #  随机梯度下降法 #  目标梯度\n$$ \\nabla_{\\omega} L=-\\sum_{x_{i} \\in M} y_{i} x_{i} $$\n$$ \\nabla_{b} L=-\\sum_{x_{i} \\in M} y_{i} $$\n选取初值$\\omega_0$, $b_0$\n迭代式\n$$ \\omega \\leftarrow \\omega+\\eta \\cdot y_{i} x_{i} $$\n$$ b \\leftarrow b+\\eta \\cdot y_{i} x_{i} $$\n初值选取和迭代过程误分类点选取不同，结果不同。\nConvergence #  存在$\\gamma\u0026gt;0$，有\n$$ y_{i}\\left ( \\omega_{opt} \\cdot x_{i}+b_{opt}\\right ) \\geqslant \\gamma $$\n感知机学习算法收敛，$R=\\max_{1 \\leqslant i \\leqslant N}\\left|x*{i}\\right|$，对误分类次数$k$，有\n$$ k \\leq\\left ( \\frac{R}{\\gamma}\\right )^{2} $$\nForm of duality #  取初值：$\\omega=\\mathit{0}, \\alpha=\\mathit{0}, b=\\mathit{0}$\n学习到的$\\omega$和$b$的形式为\n$$ \\omega=\\sum_{i=1}^{N} \\alpha_{i} \\mathcal{y}_{i} x_{i} $$\n$$ b=\\sum_{i=1}^{N} \\alpha_{i} x_{i} $$\n误分类判据变为\n$$ y_{i}\\left(\\sum_{j=1}^{N} \\alpha_{j} y_{i} x_{j} \\cdot x_{i}+b\\right) \\leq 0 $$\n迭代式\n$$ \\alpha_{i} \\leftarrow \\alpha_{i}+\\eta $$\n$$ b_{i} \\leftarrow b+\\eta y_{i} $$\n$Gram$矩阵 $= \\left[x_{i} \\cdot x_{j}\\right]_{N \\times N}$\nExample #  Show an Example: fit iris data  from typing import Tuple import numpy as np import matplotlib.pyplot as plt import time class Perceptron: def __init__(self, l_r=0.1) -\u0026gt; None: self.w = 0 self.b = 0 self.l_r = 0.1 def sign(self, x): return np.sign(np.dot(self.w, x) + self.b) def fit(self, X, Y, iter=50, w=None, b=None, l_r=None): if w is not None: if np.shape(X)[1] == len(w): self.w = w else: raise ValueError( \u0026#34;dimesion of w {} differs from X {}.\u0026#34;.format( len(w), np.shape(X)[1] ) ) else: self.w = np.zeros(np.shape(X)[1]) if b is not None: self.b = b if l_r is not None: self.l_r = l_r cur = wrong_times = 0 while cur \u0026lt; iter: id = 0 while id \u0026lt; len(X): if -1 * Y[id] * self.sign(X[id]) \u0026gt;= 0: self.w += self.l_r * np.dot(Y[id], X[id]) self.b += self.l_r * Y[id] wrong_times += 1 id += 1 cur += 1 print(\u0026#34;\\riterated {}/{} times.\u0026#34;.format(cur, iter), end=\u0026#34;\u0026#34;) print(\u0026#34;\u0026#34;, end=\u0026#34;\\n\u0026#34;) print( \u0026#34;w {}, b {}, learning_rate {}, wrong times {}\u0026#34;.format( self.w, self.b, self.l_r, wrong_times ) ) ################################################################################ # MNIST Data def load_sk_data(): import matplotlib.pyplot as plt import pandas as pd from sklearn.datasets import load_iris # load data iris = load_iris() df = pd.DataFrame(iris.data, columns=iris.feature_names) df[\u0026#34;label\u0026#34;] = iris.target df.columns = [ \u0026#34;sepal length\u0026#34;, \u0026#34;sepal width\u0026#34;, \u0026#34;petal length\u0026#34;, \u0026#34;petal width\u0026#34;, \u0026#34;label\u0026#34;, ] df.label.value_counts() data = np.array(df.iloc[:100, [0, 1, -1]]) X, y = data[:, :-1], data[:, -1] Y = np.array([1 if i == 1 else -1 for i in y]) return X, Y, data # load sk data X, Y, data = load_sk_data() # train on sk data start_time = time.time() pc = Perceptron() pc.fit(X, Y, 200) end_time = time.time() print(\u0026#34;training costs {} s\u0026#34;.format(end_time - start_time)) iterated 200/200 times. w [ 5.36 -8.13], b -4.000000000000002, learning_rate 0.1, wrong times 440 training costs 0.33515477180480957 s  x_ = np.linspace(4, 7, 10) y_ = -(pc.w[0] * x_ + pc.b) / pc.w[1] plt.plot(x_, y_) plt.plot(data[:50, 0], data[:50, 1], \u0026#34;bo\u0026#34;, color=\u0026#34;blue\u0026#34;, label=\u0026#34;0\u0026#34;) plt.plot(data[50:100, 0], data[50:100, 1], \u0026#34;bo\u0026#34;, color=\u0026#34;orange\u0026#34;, label=\u0026#34;1\u0026#34;) plt.xlabel(\u0026#34;sepal length\u0026#34;) plt.ylabel(\u0026#34;sepal width\u0026#34;) plt.legend() \u0026lt;ipython-input-6-40c643185f1c\u0026gt;:5: UserWarning: color is redundantly defined by the 'color' keyword argument and the fmt string \u0026quot;bo\u0026quot; (-\u0026gt; color='b'). The keyword argument will take precedence. plt.plot(data[:50, 0], data[:50, 1], \u0026quot;bo\u0026quot;, color=\u0026quot;blue\u0026quot;, label=\u0026quot;0\u0026quot;) \u0026lt;ipython-input-6-40c643185f1c\u0026gt;:6: UserWarning: color is redundantly defined by the 'color' keyword argument and the fmt string \u0026quot;bo\u0026quot; (-\u0026gt; color='b'). The keyword argument will take precedence. plt.plot(data[50:100, 0], data[50:100, 1], \u0026quot;bo\u0026quot;, color=\u0026quot;orange\u0026quot;, label=\u0026quot;1\u0026quot;) \u0026lt;matplotlib.legend.Legend at 0x7efcfe32d310\u0026gt;  # sklearn example from sklearn.linear_model import Perceptron # sk_pc = Perceptron(fit_intercept=True, max_iter=50, shuffle=True) # sk_pc.fit(X, Y) # print(sk_pc.coef_) # print(sk_pc.intercept_) sk_pc = Perceptron(fit_intercept=True, max_iter=100, shuffle=True) sk_pc.fit(X, Y) sk_pc.score(X, Y) 0.99    Code #  Show Code #%% from typing import Tuple import numpy as np import matplotlib.pyplot as plt import time #%% class Perceptron: def __init__(self, l_r=0.1) -\u0026gt; None: self.w = 0 self.b = 0 self.l_r = 0.1 def sign(self, x): return np.sign(np.dot(self.w, x) + self.b) def fit(self, X, Y, iter=50, w=None, b=None, l_r=None): if w is not None: if np.shape(X)[1] == len(w): self.w = w else: raise ValueError( \u0026#34;dimesion of w {} differs from X {}.\u0026#34;.format( len(w), np.shape(X)[1] ) ) else: self.w = np.zeros(np.shape(X)[1]) if b is not None: self.b = b if l_r is not None: self.l_r = l_r cur = wrong_times = 0 while cur \u0026lt; iter: id = 0 while id \u0026lt; len(X): if -1 * Y[id] * self.sign(X[id]) \u0026gt;= 0: self.w += self.l_r * np.dot(Y[id], X[id]) self.b += self.l_r * Y[id] wrong_times += 1 id += 1 cur += 1 print(\u0026#34;\\riterated {}/{} times.\u0026#34;.format(cur, iter), end=\u0026#34;\u0026#34;) print(\u0026#34;\u0026#34;, end=\u0026#34;\\n\u0026#34;) print( \u0026#34;w {}, b {}, learning_rate {}, wrong times {}\u0026#34;.format( self.w, self.b, self.l_r, wrong_times ) ) ################################################################################ # MNIST Data #%% def load_mnist_data(path) -\u0026gt; Tuple[list, list]: X = [] Y = [] with open(path, \u0026#34;r\u0026#34;) as fi: print(\u0026#34;start to read {}\u0026#34;.format(path)) for line in fi.readlines(): curLine = line.strip().split(\u0026#34;,\u0026#34;) Y.append(1 if int(curLine[0]) \u0026gt;= 5 else -1) X.append([int(num) / 255 for num in curLine[1:]]) print(\u0026#34;read end.\u0026#34;) return X, Y #%% # load mnist data X, Y = load_mnist_data( \u0026#34;/home/ias/workdir/ml-primary/ml-notes/data/Mnist/mnist_train/mnist_train.csv\u0026#34; ) X_t, Y_t = load_mnist_data( \u0026#34;/home/ias/workdir/ml-primary/ml-notes/data/Mnist/mnist_test/mnist_test.csv\u0026#34; ) # %% # train on mnist train data start_time = time.time() pc = Perceptron(l_r=0.001) pc.fit(X, Y, 50) end_time = time.time() print(\u0026#34;training costs {} s\u0026#34;.format(end_time - start_time)) #%% # test on mnist test data right = 0 wrong = 0 errors = [] for i in range(len(X_t)): y_t = 1 if Y_t[i] \u0026gt;= 5 else -1 if -1 * Y_t[i] * pc.sign(X_t[i]) \u0026lt; 0: right += 1 else: wrong += 1 print( \u0026#34;right {}, error {}, rate {}%\u0026#34;.format( right, wrong, right / (right + wrong) * 100 ) ) ################################################################################ # Sklearn iris data #%% def load_sk_data(): import matplotlib.pyplot as plt import pandas as pd from sklearn.datasets import load_iris # load data iris = load_iris() df = pd.DataFrame(iris.data, columns=iris.feature_names) df[\u0026#34;label\u0026#34;] = iris.target df.columns = [ \u0026#34;sepal length\u0026#34;, \u0026#34;sepal width\u0026#34;, \u0026#34;petal length\u0026#34;, \u0026#34;petal width\u0026#34;, \u0026#34;label\u0026#34;, ] df.label.value_counts() data = np.array(df.iloc[:100, [0, 1, -1]]) X, y = data[:, :-1], data[:, -1] Y = np.array([1 if i == 1 else -1 for i in y]) return X, Y, data # %% # load sk data X, Y, data = load_sk_data() # %% # train on sk data start_time = time.time() pc = Perceptron() pc.fit(X, Y, 200) end_time = time.time() print(\u0026#34;training costs {} s\u0026#34;.format(end_time - start_time)) #%% x_ = np.linspace(4, 7, 10) y_ = -(pc.w[0] * x_ + pc.b) / pc.w[1] plt.plot(x_, y_) plt.plot(data[:50, 0], data[:50, 1], \u0026#34;bo\u0026#34;, color=\u0026#34;blue\u0026#34;, label=\u0026#34;0\u0026#34;) plt.plot(data[50:100, 0], data[50:100, 1], \u0026#34;bo\u0026#34;, color=\u0026#34;orange\u0026#34;, label=\u0026#34;1\u0026#34;) plt.xlabel(\u0026#34;sepal length\u0026#34;) plt.ylabel(\u0026#34;sepal width\u0026#34;) plt.legend() # %% # sklearn example from sklearn.linear_model import Perceptron # sk_pc = Perceptron(fit_intercept=True, max_iter=50, shuffle=True) # sk_pc.fit(X, Y) # print(sk_pc.coef_) # print(sk_pc.intercept_) sk_pc = Perceptron(fit_intercept=True, max_iter=100, shuffle=True) sk_pc.fit(X, Y) sk_pc.score(X, Y) # %%    "})})()